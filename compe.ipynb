{"cells":[{"cell_type":"markdown","metadata":{"id":"lHJSLwLyBjnp"},"source":["# AILab 画像分類コンペティション\n","\n","## ルール\n","検証にCIFAR10のValデータは使わないでください。\n","\n","(今回評価用のデータとして用いるため)\n","\n","モデルを評価する際は、Trainデータを分割して使ってください。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTjjoBfyBjny"},"outputs":[],"source":["#ライブラリのインポート\n","import torch as torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from tqdm import tqdm\n","from torchsummary import summary\n","from pylab import rcParams\n","\n","from utils import make_dataset\n","from utils import eval"]},{"cell_type":"markdown","metadata":{"id":"-I3-z91cBjn1"},"source":["### 1.1. データセットの読み込み（改変不可）\n","- pytorchではデータをTensor形式で扱います。(listやnumpyと似たような構造を持つ)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtJ60dgnBjn2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651902893979,"user_tz":-540,"elapsed":6199,"user":{"displayName":"-_- pLm","userId":"05682436097312232603"}},"outputId":"cd33cb23-8d9e-420b-ffbe-ca380f34350e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6150787d6f25488fb325e6dcac7ad6d3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./dataset/cifar-10-python.tar.gz to ./dataset/\n"]}],"source":["#このセルは改変しないで\n","train_dataset=make_dataset()"]},{"cell_type":"markdown","metadata":{"id":"pJFiltYYBjoC"},"source":["### 1.2. データセットの可視化"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMwzNN_IBjoD"},"outputs":[],"source":["%matplotlib inline\n","def show(img):\n","    npimg = img.numpy() \n","    plt.grid(False) \n","    plt.imshow(np.transpose(npimg, (1,2,0)))\n","    plt.show()\n","\n","print (train_dataset)\n","image, label = train_dataset[1]\n","print(\"学習画像サイズ\",image.size())\n","print (\"ラベル:\",label)\n","show(image)"]},{"cell_type":"markdown","metadata":{"id":"GFvZRjDUBjoH"},"source":["### 1.3. データセットをミニバッチ単位に変換します。\n","- **torch.utils.data.DataLoader**クラスで行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HhV3WIsBjoI"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=200, #バッチサイズの定義\n","                                           shuffle=True,\n","                                          num_workers=1)\n"]},{"cell_type":"markdown","metadata":{"id":"zKkA-yOtBjoL"},"source":["## 2.モデルの定義\n","### 2.1. モデルの定義(nn.Moduleを継承したクラスでの書き方)\n","- **__init__**関数でネットワーク層の定義、初期化を行います。\n","- **forward**関数でモデルの入力→出力の定義、順伝搬の計算を行います。  \n","    - 定義したネットワーク層に入力(x)を伝搬していきます。\n","- 全結合ネットワークは**nn.Liner(入力パラメータ数、出力パラメータ数)**で定義します。\n","    - 画像データを扱う場合、入力パラメータ数は**二次元データを一次元に変換**して入力します。\n","    - Tensorのサイズ変換には**Tensor.view(軸について要素数を指定)**を用います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL3Vw4hm-IX0"},"outputs":[],"source":["class atte_Block(nn.Module):\n","    def __init__(self, channels_in, channels_out):\n","        super(atte_Block, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channels_in, channels_out, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(channels_out),\n","            nn.GELU(),\n","        )\n","        self.atte = nn.Sequential(\n","            nn.Conv2d(channels_out, channels_out, kernel_size=3, padding=1),\n","            nn.GELU(),\n","            nn.Conv2d(channels_out, channels_out, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        h = self.features(x)\n","        h_atte=self.atte(h)\n","        y=h*h_atte\n","        return y\n","\n","class cnn_Block(nn.Module):\n","    def __init__(self, inp, oup):\n","        super(cnn_Block, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(inp, oup, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(oup),\n","            nn.GELU()\n","        )\n","    \n","    def forward(self, x):\n","        h = self.features(x)\n","        y = h + x\n","        return y\n","    \n","class atte_CNN(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(atte_CNN, self).__init__()\n","        num=32\n","        self.atte_block0 = atte_Block(input_dim, num) \n","        self.blocks0 = nn.ModuleList([cnn_Block(num, num) for _ in range(3)])\n","        self.maxpool0 = nn.MaxPool2d(kernel_size=2)\n","        self.atte_block1 = atte_Block(num, num) \n","        self.blocks1 = nn.ModuleList([cnn_Block(num, num) for _ in range(3)])\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n","        self.full = nn.Sequential(\n","            nn.Linear(16*16*num, 4096),\n","            nn.GELU(),\n","            nn.Linear(4096, output_dim),\n","        )\n","        \n","    def forward(self, x):\n","        h = self.atte_block0(x)\n","        for block in self.blocks0:\n","            h = block(h)\n","        h = self.maxpool0(h)\n","        h = self.atte_block1(h)\n","        for block in self.blocks1:\n","            h = block(h)\n","        h = self.maxpool1(h)\n","        h = h.view(x.size(0),-1)\n","        y = self.full(h)\n","        return y \n","\n","#print(atte_CNN(3, 10))\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' #GPUの定義\n","net = atte_CNN(3, 10).to(device)"]},{"cell_type":"markdown","metadata":{"id":"FLrQHdL7BjoQ"},"source":["### 2.2. モデルの可視化\n","- summary(モデル,入力サイズ(チャンネル数、パラメータ数))クラスで可視化を行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAHTUjZrBjoQ"},"outputs":[],"source":["summary(net, input_size=(3,64,64))"]},{"cell_type":"markdown","metadata":{"id":"GkOyYt_TBjoT"},"source":["## 3. モデルの学習\n","### 3.1. 損失関数、最適化法(オプティマイザー)の定義\n","- 損失関数は**nn.CrossEntropyLoss()**(ソフトマックス+交差エントロピー)を用います。\n","- 最適化法には**optim.SGD(適用するモデルの重み、その他ハイパーパラメータの設定・・・)**を用います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsYsLJdyBjoU"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001,  weight_decay=5e-4)"]},{"cell_type":"markdown","metadata":{"id":"AyXqyVoyBjoX"},"source":["### 3.2. 学習の流れ(エポック単位)\n","- ①画像データと正解ラベルを取り出す\n","    - tqdmはjupyter内にプログレスバー(進行度)を表示する\n","- ②画像データとラベルデータ(共にバッチ単位)をデバイス(GPU)に移動\n","- ③勾配の初期化\n","- ④モデルの推測(画像データ→各クラスである確率)\n","- ⑤損失の計算\n","- ⑥⑤で求めた損失をエポックでの累計損失に加える\n","    - Tensorから値を取得する場合は、1次元(1要素)に指定してから.item()で取得する必要がある\n","- ⑦正答率の計算(出力の最大値のインデックスが正解ラベルの場合1を出力をバッチサイズ分行う(.sum())\n","    - (.maxは()で比較する値の軸を選び、[0]に最大値の値、[1]に最大値のインデックスを持つ)\n","- ⑧逆伝搬の計算\n","- ⑨勾配の更新<br>\n","\n","---全バッチで行う---\n","- ⑩エポックでの平均誤差の計算\n","- ⑪エポックでの平均正答率の計算\n","- 検証データでの損失、精度を学習時と同じアルゴリズムで検証(逆伝搬、勾配の更新は行わない)\n","\n","*1:学習時には.train()で学習に関連する機能を有効にします。   \n","*2:検証時には.eval()で学習に関連する機能を有効に、torch.no_grad()で自動微分を停止してから検証を行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnxAYcG8BjoY","outputId":"4e3ee53d-c5d7-4f08-bf10-63caee5257d6","executionInfo":{"status":"ok","timestamp":1651913337050,"user_tz":-540,"elapsed":680618,"user":{"displayName":"-_- pLm","userId":"05682436097312232603"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch * 1\n","Loss: 0.0147, acc: 36.9860%\n","epoch * 2\n","Loss: 0.0063, acc: 54.7860%\n","epoch * 3\n","Loss: 0.0050, acc: 64.0620%\n","epoch * 4\n","Loss: 0.0041, acc: 70.6980%\n","epoch * 5\n","Loss: 0.0034, acc: 76.0820%\n"]}],"source":["num_epochs = 5 #学習エポックの設定\n","\n","#グラフ作成のためにエポックごとの各値を保存する配列を作成\n","train_loss_list = []\n","train_acc_list = []\n","\n","\n","#学習の定義\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    train_acc = 0\n","    i=0#学習回数\n","    \n","    net.train() #train *1\n","    for (images, labels) in train_loader: #①\n","        images, labels = images.to(device), labels.to(device) #②\n","        optimizer.zero_grad() #③\n","        outputs = net(images) #④\n","        loss = criterion(outputs, labels)#⑤\n","        train_loss += loss.item()#⑥\n","        train_acc += (outputs.max(1)[1] == labels).sum().item() #⑦\n","        loss.backward()#⑧\n","        optimizer.step()#⑨\n","        \n","        \n","    \n","    avg_train_loss = train_loss / len(train_loader.dataset) #⑩\n","    avg_train_acc = train_acc / len(train_loader.dataset)   #⑪\n","    \n","    print('epoch *', (epoch+1))\n","    print('Loss: {:.4f}, acc: {:.4f}%'.format(avg_train_loss, 100*avg_train_acc))\n","    #グラフ壁画用に各値を配列に格納\n","    train_loss_list.append(avg_train_loss)\n","    train_acc_list.append(avg_train_acc)"]},{"cell_type":"markdown","metadata":{"id":"CAbATIl2Bjob"},"source":["## 4. 学習記録のグラフ化\n","- matplotlibを用います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlc4MPFzBjod"},"outputs":[],"source":["#学習のグラフ化\n","plt.figure()\n","plt.plot(range(num_epochs), train_loss_list, color='blue', linestyle='-', label='train_loss')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.title('Training loss')\n","plt.grid()\n","\n","plt.figure()\n","plt.plot(range(num_epochs), train_acc_list, color='blue', linestyle='-', label='train_acc')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('acc')\n","plt.title('Training accuracy')\n","plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17we2AKLobId"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"Jgtfwf0IobId"},"source":["## 評価用セル(改変不可)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5pLYd73obIe","executionInfo":{"status":"ok","timestamp":1651913443163,"user_tz":-540,"elapsed":10411,"user":{"displayName":"-_- pLm","userId":"05682436097312232603"}},"outputId":"975c958b-5644-414d-eba1-b33de2a6b417"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["スコア 0.7106\n"]}],"source":["score=eval(net)\n","print(\"スコア\",score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvLFiq1yobIe"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"compe.ipynb のコピー","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}